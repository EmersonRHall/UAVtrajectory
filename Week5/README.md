This is week 5 of the research.  
This week focused on selecting and designing a machine learning model for UAV trajectory optimization. After considering several options, Proximal Policy Optimization (PPO) was chosen because it supports continuous control and stable learning, making it well-suited for smooth UAV motion. Deep Q Networks were noted as a possible discrete-action baseline, while Long Short-Term Memory layers may be added later to capture time dependencies.
The PPO model takes as input the UAVâ€™s relative position to the goal, velocity components, obstacle distances, and a local occupancy grid for environmental awareness. It outputs continuous control signals for velocity or waypoint adjustments, allowing safe and efficient trajectory generation. The network architecture includes dense hidden layers with nonlinear activation functions, an optional LSTM layer, and an actor-critic structure with separate policy and value heads.
Training will rely on a carefully designed reward function that encourages goal progress, penalizes collisions, and promotes smooth paths through penalties on sharp turns. Randomized obstacle placements in simulation will be used to ensure generalization. This design provides the foundation for training and evaluating reinforcement learning models in later stages of the project.
